{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f2d6a3",
   "metadata": {},
   "source": [
    "## Step 0: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q opencv-python albumentations torch torchvision tqdm pandas numpy scikit-learn matplotlib seaborn pillow\n",
    "\n",
    "print(\"✓ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b02d96",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive and Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb99c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set your dataset path (MODIFY THIS PATH TO YOUR DATASET LOCATION)\n",
    "DATASET_BASE_PATH = '/content/drive/MyDrive/Leaf Disease Detection'\n",
    "\n",
    "# Change working directory\n",
    "os.chdir(DATASET_BASE_PATH)\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"\\nDataset folders:\")\n",
    "if os.path.exists('Dataset'):\n",
    "    print(os.listdir('Dataset'))\n",
    "else:\n",
    "    print(\"⚠️ Dataset folder not found! Please upload your Dataset folder to Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f5411",
   "metadata": {},
   "source": [
    "---\n",
    "# PART A: DATA PREPROCESSING\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f4d37",
   "metadata": {},
   "source": [
    "## A1. Import Libraries for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19460a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a65fa1",
   "metadata": {},
   "source": [
    "## A2. Configuration for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e9b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASET_PATH = \"Dataset\"\n",
    "OUTPUT_PATH = \"Dataset_Resized\"\n",
    "CSV_OUTPUT = \"dataset_info.csv\"\n",
    "IMG_SIZE = 224\n",
    "AUGMENTATIONS_PER_IMAGE = 2\n",
    "\n",
    "# Class folders\n",
    "CLASS_FOLDERS = [\n",
    "    \"Bacterial Leaf Spot\",\n",
    "    \"Downy Mildew\",\n",
    "    \"Healthy Leaf\",\n",
    "    \"Mosaic Disease\",\n",
    "    \"Powdery_Mildew\"\n",
    "]\n",
    "\n",
    "print(f\"Image resize target: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Augmentations per image: {AUGMENTATIONS_PER_IMAGE}\")\n",
    "print(f\"Classes: {len(CLASS_FOLDERS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d394c",
   "metadata": {},
   "source": [
    "## A3. Define Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmentation_pipeline():\n",
    "    \"\"\"Create augmentation pipeline using albumentations\"\"\"\n",
    "    return A.Compose([\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.3),\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.2, \n",
    "            contrast_limit=0.2, \n",
    "            p=0.5\n",
    "        ),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.05,\n",
    "            scale_limit=0.1,\n",
    "            rotate_limit=15,\n",
    "            p=0.5\n",
    "        ),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(p=1),\n",
    "            A.GaussianBlur(p=1),\n",
    "        ], p=0.3),\n",
    "    ])\n",
    "\n",
    "def resize_image(image, size=224):\n",
    "    \"\"\"Resize image to specified size\"\"\"\n",
    "    return cv2.resize(image, (size, size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def save_image(image, path):\n",
    "    \"\"\"Save image to specified path\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    cv2.imwrite(path, image)\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc936d51",
   "metadata": {},
   "source": [
    "## A4. Process Dataset (Augmentation + Resizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195da5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset():\n",
    "    \"\"\"Main processing function\"\"\"\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    print(f\"Target image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "    print(f\"Augmentations per image: {AUGMENTATIONS_PER_IMAGE}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "    # Initialize augmentation\n",
    "    augment = create_augmentation_pipeline()\n",
    "    \n",
    "    # Data storage\n",
    "    data_records = []\n",
    "    \n",
    "    # Process each class\n",
    "    for class_idx, class_name in enumerate(CLASS_FOLDERS):\n",
    "        print(f\"\\nProcessing class: {class_name} (Label: {class_idx})\")\n",
    "        \n",
    "        class_input_path = os.path.join(DATASET_PATH, class_name)\n",
    "        class_output_path = os.path.join(OUTPUT_PATH, class_name)\n",
    "        \n",
    "        # Check if class folder exists\n",
    "        if not os.path.exists(class_input_path):\n",
    "            print(f\"Warning: Folder {class_input_path} not found. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Get all images in class folder\n",
    "        image_files = [f for f in os.listdir(class_input_path) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        print(f\"Found {len(image_files)} images\")\n",
    "        \n",
    "        # Process each image\n",
    "        for img_idx, img_file in enumerate(tqdm(image_files, desc=f\"Processing {class_name}\")):\n",
    "            original_path = os.path.join(class_input_path, img_file)\n",
    "            \n",
    "            # Read original image\n",
    "            img = cv2.imread(original_path)\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read {original_path}. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            original_height, original_width = img.shape[:2]\n",
    "            \n",
    "            # Save original resized image (no augmentation)\n",
    "            img_resized = resize_image(img, IMG_SIZE)\n",
    "            base_name = Path(img_file).stem\n",
    "            ext = Path(img_file).suffix\n",
    "            \n",
    "            output_filename = f\"{base_name}_orig{ext}\"\n",
    "            output_path = os.path.join(class_output_path, output_filename)\n",
    "            save_image(img_resized, output_path)\n",
    "            \n",
    "            # Record original image data\n",
    "            data_records.append({\n",
    "                'filename': output_filename,\n",
    "                'original_filename': img_file,\n",
    "                'class_name': class_name,\n",
    "                'class_label': class_idx,\n",
    "                'original_width': original_width,\n",
    "                'original_height': original_height,\n",
    "                'resized_width': IMG_SIZE,\n",
    "                'resized_height': IMG_SIZE,\n",
    "                'augmentation_type': 'original',\n",
    "                'relative_path': os.path.join(class_name, output_filename),\n",
    "                'original_image_path': original_path\n",
    "            })\n",
    "            \n",
    "            # Create augmented versions\n",
    "            for aug_idx in range(AUGMENTATIONS_PER_IMAGE):\n",
    "                # Apply augmentation\n",
    "                augmented = augment(image=img)\n",
    "                img_aug = augmented['image']\n",
    "                \n",
    "                # Resize augmented image\n",
    "                img_aug_resized = resize_image(img_aug, IMG_SIZE)\n",
    "                \n",
    "                # Save augmented image\n",
    "                aug_filename = f\"{base_name}_aug{aug_idx+1}{ext}\"\n",
    "                aug_output_path = os.path.join(class_output_path, aug_filename)\n",
    "                save_image(img_aug_resized, aug_output_path)\n",
    "                \n",
    "                # Record augmented image data\n",
    "                data_records.append({\n",
    "                    'filename': aug_filename,\n",
    "                    'original_filename': img_file,\n",
    "                    'class_name': class_name,\n",
    "                    'class_label': class_idx,\n",
    "                    'original_width': original_width,\n",
    "                    'original_height': original_height,\n",
    "                    'resized_width': IMG_SIZE,\n",
    "                    'resized_height': IMG_SIZE,\n",
    "                    'augmentation_type': f'augmented_{aug_idx+1}',\n",
    "                    'relative_path': os.path.join(class_name, aug_filename),\n",
    "                    'original_image_path': original_path\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    print(f\"\\nCreating CSV file: {CSV_OUTPUT}\")\n",
    "    df = pd.DataFrame(data_records)\n",
    "    df.to_csv(CSV_OUTPUT, index=False)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PREPROCESSING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total images processed: {len(df)}\")\n",
    "    print(f\"CSV file saved: {CSV_OUTPUT}\")\n",
    "    print(f\"Resized dataset folder: {OUTPUT_PATH}\")\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(df['class_name'].value_counts().sort_index())\n",
    "    print(\"\\nAugmentation distribution:\")\n",
    "    print(df['augmentation_type'].value_counts())\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run preprocessing\n",
    "df = process_dataset()\n",
    "print(\"\\n✓ Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cabeed",
   "metadata": {},
   "source": [
    "---\n",
    "# PART B: MODEL TRAINING & EVALUATION\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f088d35",
   "metadata": {},
   "source": [
    "## B1. Import Training Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69654310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb5cea",
   "metadata": {},
   "source": [
    "## B2. Load and Split Dataset (70/15/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv('dataset_info.csv')\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['class_name'].value_counts())\n",
    "\n",
    "# Split dataset: 70% train, 15% validation, 15% test\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.3, stratify=df['class_label'], random_state=42\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df['class_label'], random_state=42\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTrain: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Val: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46581937",
   "metadata": {},
   "source": [
    "## B3. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeafDiseaseDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir='Dataset_Resized', transform=None):\n",
    "        self.df = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.df.loc[idx, 'relative_path'])\n",
    "        label = self.df.loc[idx, 'class_label']\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = LeafDiseaseDataset(train_df, transform=transform)\n",
    "val_dataset = LeafDiseaseDataset(val_df, transform=transform)\n",
    "test_dataset = LeafDiseaseDataset(test_df, transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"✓ DataLoaders created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec05eae",
   "metadata": {},
   "source": [
    "## B4. Define AlexNet Simplified Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4030edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet_simplified(nn.Module):\n",
    "    def __init__(self, num_classes=5, exit_threshold=0.90):\n",
    "        super(AlexNet_simplified, self).__init__()\n",
    "        self.exit_threshold = exit_threshold\n",
    "\n",
    "        # Conv Layer 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "\n",
    "        # Conv Layer 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "\n",
    "        self.exit1 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        # Conv Layer 3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.exit2 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(384, num_classes)\n",
    "        )\n",
    "\n",
    "        # Conv Layer 4\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.exit3 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(384, num_classes)\n",
    "        )\n",
    "\n",
    "        # Conv Layer 5\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        # Final Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, inference=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        out1 = self.exit1(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        out2 = self.exit2(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        out3 = self.exit3(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.avgpool(x)\n",
    "        out_final = self.classifier(x)\n",
    "\n",
    "        if inference:\n",
    "            conf1 = F.softmax(out1, dim=1).max(1).values\n",
    "            if conf1.item() >= self.exit_threshold:\n",
    "                return out1, \"Exit1\"\n",
    "\n",
    "            conf2 = F.softmax(out2, dim=1).max(1).values\n",
    "            if conf2.item() >= self.exit_threshold:\n",
    "                return out2, \"Exit2\"\n",
    "\n",
    "            conf3 = F.softmax(out3, dim=1).max(1).values\n",
    "            if conf3.item() >= self.exit_threshold:\n",
    "                return out3, \"Exit3\"\n",
    "\n",
    "            return out_final, \"Final\"\n",
    "\n",
    "        return out1, out2, out3, out_final\n",
    "\n",
    "# Initialize model\n",
    "model = AlexNet_simplified(num_classes=5, exit_threshold=0.90).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31dc873",
   "metadata": {},
   "source": [
    "## B5. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651603a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "EARLY_STOP_PATIENCE = 7\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: 0.001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe81f57",
   "metadata": {},
   "source": [
    "## B6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e8d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out1, out2, out3, out_final = model(images, inference=False)\n",
    "        \n",
    "        loss1 = criterion(out1, labels)\n",
    "        loss2 = criterion(out2, labels)\n",
    "        loss3 = criterion(out3, labels)\n",
    "        loss_final = criterion(out_final, labels)\n",
    "        \n",
    "        loss = 0.1 * loss1 + 0.2 * loss2 + 0.3 * loss3 + 0.4 * loss_final\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(out_final.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            out1, out2, out3, out_final = model(images, inference=False)\n",
    "            loss = criterion(out_final, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(out_final.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18372e69",
   "metadata": {},
   "source": [
    "## B7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14928eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"✓ Model saved!\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"Early stopping: {early_stop_counter}/{EARLY_STOP_PATIENCE}\")\n",
    "        \n",
    "    if early_stop_counter >= EARLY_STOP_PATIENCE:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n✓ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17724bac",
   "metadata": {},
   "source": [
    "## B8. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de803c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(train_losses, label='Train Loss', marker='o')\n",
    "axes[0].plot(val_losses, label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(train_accs, label='Train Acc', marker='o')\n",
    "axes[1].plot(val_accs, label='Val Acc', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best validation accuracy: {max(val_accs):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8b957",
   "metadata": {},
   "source": [
    "## B9. Test on Original Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "def predict_original_image(model, image_path, device):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img, (224, 224))\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    img_tensor = transform(img_resized).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out1, out2, out3, out_final = model(img_tensor, inference=False)\n",
    "        probabilities = F.softmax(out_final, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "        \n",
    "    return predicted.item(), confidence.item(), probabilities.cpu().numpy()[0], img\n",
    "\n",
    "# Test on original images\n",
    "test_original_images = test_df[test_df['augmentation_type'] == 'original'].copy()\n",
    "\n",
    "class_names = [\n",
    "    \"Bacterial Leaf Spot\",\n",
    "    \"Downy Mildew\",\n",
    "    \"Healthy Leaf\",\n",
    "    \"Mosaic Disease\",\n",
    "    \"Powdery Mildew\"\n",
    "]\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "confidences = []\n",
    "\n",
    "for idx, row in tqdm(test_original_images.iterrows(), total=len(test_original_images), desc=\"Testing\"):\n",
    "    pred_label, confidence, probs, _ = predict_original_image(model, row['original_image_path'], device)\n",
    "    predictions.append(pred_label)\n",
    "    true_labels.append(row['class_label'])\n",
    "    confidences.append(confidence)\n",
    "\n",
    "test_accuracy = accuracy_score(true_labels, predictions) * 100\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"Average Confidence: {np.mean(confidences)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6096cd",
   "metadata": {},
   "source": [
    "## B10. Confusion Matrix and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956b74c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(true_labels, predictions, target_names=class_names))\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - Test on Original Images', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad652ec8",
   "metadata": {},
   "source": [
    "## B11. Visualize Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c458c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = np.random.choice(len(test_original_images), size=min(9, len(test_original_images)), replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    row = test_original_images.iloc[idx]\n",
    "    pred_label, confidence, probs, img = predict_original_image(model, row['original_image_path'], device)\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "    color = 'green' if pred_label == row['class_label'] else 'red'\n",
    "    title = f\"True: {class_names[row['class_label']]}\\n\"\n",
    "    title += f\"Pred: {class_names[pred_label]}\\n\"\n",
    "    title += f\"Conf: {confidence*100:.1f}%\"\n",
    "    axes[i].set_title(title, fontsize=10, color=color, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Sample Predictions on Original Test Images', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ee602",
   "metadata": {},
   "source": [
    "## B12. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: AlexNet Simplified with Early Exits\")\n",
    "print(f\"Classes: {len(class_names)}\")\n",
    "print(f\"Input Size: 224x224x3\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nDataset Sizes:\")\n",
    "print(f\"  Train: {len(train_df)}\")\n",
    "print(f\"  Validation: {len(val_df)}\")\n",
    "print(f\"  Test: {len(test_df)}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Best Val Accuracy: {max(val_accs):.2f}%\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"  Avg Confidence: {np.mean(confidences)*100:.2f}%\")\n",
    "print(\"\\nClass-wise Accuracy:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_mask = np.array(true_labels) == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = accuracy_score(\n",
    "            np.array(true_labels)[class_mask], \n",
    "            np.array(predictions)[class_mask]\n",
    "        ) * 100\n",
    "        print(f\"  {class_name}: {class_acc:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n✓ All tasks completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
